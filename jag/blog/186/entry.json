{
  "date" : 1151046000000,
  "title" : "MPI meets Multicore",
  "body" : "Last month I had the great thrill of being asked to give a talk at\nthe <a href=\"http://spie.org/conferences/programs/06/as/\">SPIE\nAstronomy Conference</a> (thanks to Hilton Lewis for inviting me).\nI gave a broad talk on the state of scientific computing. The issue\nI got the most feedback on is an odd stress I've noticed between <a\nhref=\n\"http://en.wikipedia.org/wiki/Message_Passing_Interface\">MPI</a>\nand the proliferation of multicore chips. It goes like this: \n\n<p>MPI is a Message Passing Interface that is the predominant API\nused in building applications for scientific clusters. There are\nJava versions; one of my favorites is an MPI-like system called <a\nhref=\"http://dsg.port.ac.uk/projects/mpj\">MPJ Express</a>. MPI has\nbeen around for years and there's a large body of existing MPI\napplications in C and Fortran.</p>\n\n<p>On the one hand, MPI is a wonderful thing because it enables\nscientific applications to take advantage of large clusters. On the\nother hand it's horrible because it forces applications to ship\nlarge quantities of data around, and they often quickly find that\ncommunication overheads far overshadow the actual computation. An\nimmense amount of cleverness goes into both making the\ncommunication efficient, and minimizing the amount of\ncommunication. For example, computational fluid dynamics\napplications will sometimes do adaptive non-uniform partitioning of\nthe simulation volume so that regions with highly detailed\ninteractions (like vortices) don't get split across computation\nnodes. Scientific applications generally get much better CPU\nutilization if they are run in one big address space with all of\nthe CPUs having direct access to it.</p>\n\n<p>But there's a huge catch: these applications are mostly single\nthreaded, having been designed for clusters where each node is a\nsingle-CPU machine. But now the shift to multicore is happening and\nfolks are building clusters of SMPs. There is a tendancy to run\nmultiple copies of the application on each machine, one per core.\nUsing MPI within one machine to communicate between the copies of\nthe application. This is crazy - especially on machines with lots\nof cores: which will be the norm in not too many years. Even though\nthe communication is often highly optimized within the one machine,\nthere's still overhead.</p>\n\n<p>To get the best CPU utilization the apps have to be written to\nbe multithreaded on a cluster node, and use MPI between nodes. This\nis the worst of both worlds because you have to architect for\nthreading <i>and</i> clustering at the same time. This is pretty\nstraightforward in the Java world because we have great threading\nfacilities, but folks with bags of Fortran code have trouble\n(auto-parallelizing matrix code doesn't help nearly enough).</p>\n\n<p>This is (almost) a non-issue for folks writing enterprise\napplications using the JavaEE frameworks because the containers\ndeal with both threading and clustering.</p>\n\n",
  "images" : [ ]
}