{
  "date" : 1122447600000,
  "title" : "Transcendental Meditation",
  "body" : "I got into a conversation with some folks who've been moving a\nlarge sophisticated image processing application to Java. They've\nbeen getting great performance numbers, much to the surprise of the\nC crowd in their shop. \n\n<p style=\"text-align: left\">With one exception: code that invokes\nsin() and cos() heavily is somewhat slower. They asked me why this\nwas happening. I had a pretty good idea, but I checked with Joe\nDarcy, our local Floating Point God, and he had this to say:</p>\n\n<p style=\"text-align: left\"><i>For many years, the JDK on x86\nplatforms has used the hardware fsin/fcos x87 instructions in the\nrange [-</i>pi<i>/4,</i> pi<i>/4], a range which encompasses about\nhalf of all representable floating-point values. Therefore, in that\nrange the performance of the JDK's transcendental functions should\nbe nearly the same as the performance of the transcendental\nfunctions in C, C++, etc. that are using those same fsin/fcos\ninstructions. Benchmarks which focus on testing the trig\nperformance of large values, such as almabench, present a skewed\nportrait of Java's trigonometric performance. The next question is\nwhy don't we just use fsin/fcos over the entire floating-point\nrange? The simple answer is that fsin/fcos can deliver answers that\nare arbitrarily wrong for the most straightforward way of measuring\nerror in the result.</i></p>\n\n<p style=\"text-align: left\"><i>Every finite real number, no matter\nhow large, has a well-defined value for sin/cos. Ideally, the\nfloating-point result returned for sin/cos would be the\nrepresentable floating-point number closest to the mathematically\ndefined result for the floating-point input. A floating-point\nlibrary having this property is called correctly rounded, which is\nequivalent to saying the library has an error bound less than or\nequal to 1/2 an ulp (unit in the last place). For sin/cos, writing\na correctly rounding implementation that runs at a reasonable speed\nis still something of a research problem so in practice platforms\noften use a library with a 1 ulp error bound instead, which means\neither of the floating-point numbers adjacent to the true result\ncan be returned. This is the implementation criteria the Java Math\nlibrary has to meet. The implementation challenge is that sin/cos\nare implemented using argument reduction whereby any input is\nmapped into a corresponding input in the [-</i>pi<i>/4,</i>\npi<i>/4] range. Since the period of sin/cos is</i> pi <i>and</i> pi\n<i>is transcendental, this amounts to having to compute a remainder\nfrom the division by a transcendental number, which is non-obvious.\nA few years after the x87 was designed, people figured out how to\ndo this division as if by an exact value of</i> pi<i>. Instead the\nx87 fsin/fcos use a particular approximation to</i> pi<i>, which\neffectively means the period of the function is changed, which can\nlead to large errors outside [-</i>pi<i>/4,</i> pi<i>/4]. For\nexample the value of sine for the floating-point number Math.PI is\naround</i></p>\n\n<p style=\"margin-left: 30.0; text-align: left\">\n<i>1.2246467991473532E-16</i></p>\n\n<p style=\"text-align: left\"><i>while the computed value from fsin\nis</i></p>\n\n<p style=\"margin-left: 30.0; text-align: left\"><i>1.2246<span\nstyle=\"color: #CC0000\">063538223773E-16</span></i></p>\n\n<p style=\"text-align: left\"><i>In other words, instead of getting\nthe full 15-17 digit accuracy of double, the returned result is\nonly correct to about 5 decimal digits. In terms of ulps, the error\nis about 1.64e11 ulps, over *ten billion* ulps. With some effort,\nI'm confident I could find results with the wrong sign, etc. There\nis a rationale which can justify this behavior; however, it was\nmuch more compelling before the argument reduction problem was\nsolved.</i></p>\n\n<p style=\"text-align: left\">This error has tragically become\nun-fixable because of the compatibility requirements from one\ngeneration to the next. The fix for this problem was figured out\nquite a long time ago. In the excellent paper <i><a href=\n\"http://doi.ieeecomputersociety.org/10.1109/ARITH.1995.465368\"\nshape=\"rect\">The K5 transcendental functions</a></i> by T. Lynch,\nA. Ahmed, M. Schulte, T. Callaway, and R. Tisdale a technique is\ndescribed for doing argument reduction as if you had an infinitely\nprecise value for pi. As far as I know, the K5 is the only x86\nfamily CPU that did sin/cos accurately. AMD went back to being\nbit-for-bit compatibile with the old x87 behavior, assumably\nbecause too many applications broke. Oddly enough, this is fixed in\nItanium.</p>\n\n<p style=\"text-align: left\">What we do in the JVM on x86 is\nmoderately obvious: we range check the argument, and if it's\noutside the range [-<i>pi</i>/4, <i>pi</i>/4]we do the precise\nrange reduction by hand, and then call fsin.</p>\n\n<p style=\"text-align: left\">So Java is accurate, but slower. I've\nnever been a fan of \"fast, but wrong\" when \"wrong\" is roughly\nrandom(). Benchmarks rarely test accuracy. \"double sin(double\ntheta) { return 0; }\" would be a great benchmark-compatible\nimplementation of sin(). For large values of theta, 0 would be\narguably more accurate since the absolute error is never greater\nthan 1. fsin/fcos can have absolute errors as large as 2 (correct\nanswer=1; returned result=-1).</p>\n\n<p style=\"text-align: left\">This is one of those area where no\nmatter what we do, we're screwed.</p>\n\n",
  "images" : [ {
    "image" : "2005_07_27_10-42-09-387_n1.small.png",
    "href" : null,
    "align" : "right",
    "width" : 200,
    "height" : 135
  } ]
}